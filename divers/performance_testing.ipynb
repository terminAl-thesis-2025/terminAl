{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T08:12:00.998322Z",
     "start_time": "2025-05-01T08:12:00.992660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from icecream import ic\n",
    "from collections import deque\n"
   ],
   "id": "f27622864c52afc6",
   "outputs": [],
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-01T08:12:01.161175Z",
     "start_time": "2025-05-01T08:12:01.122224Z"
    }
   },
   "source": [
    "class SystemMapping:\n",
    "    settings = json.load(open(\"../settings/settings.json\"))\n",
    "\n",
    "    @classmethod\n",
    "    def map_all(cls):\n",
    "        psql_results = cls.map_postgres()\n",
    "        os_results = cls.map_os()\n",
    "\n",
    "        return os_results, psql_results\n",
    "\n",
    "    @classmethod\n",
    "    def map_postgres(cls):\n",
    "        postgres_settings = cls.settings.get(\"tools\", \"\").get(\"postgres\", \"\")\n",
    "        username = postgres_settings.get(\"username\", \"\")\n",
    "        databases = postgres_settings.get(\"databases\", [])  # list of Databses\n",
    "        mapping_tables_command = postgres_settings.get(\"mapping_tables_command\", [])  # list of command parts\n",
    "\n",
    "        table_results = []\n",
    "\n",
    "        mapping_tables_command[2] = username\n",
    "\n",
    "\n",
    "        if databases and mapping_tables_command and username:\n",
    "            for database in databases:\n",
    "                mapping_tables_command[5] = database\n",
    "\n",
    "                try:\n",
    "                    # Store the result of subprocess.run()\n",
    "                    result = subprocess.run(\n",
    "                        mapping_tables_command,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        check=True\n",
    "                    )\n",
    "\n",
    "                    # Append both database name and command output to psql_results\n",
    "                    table_results.append({\n",
    "                        \"database\": database,\n",
    "                        \"tables_table\": result.stdout\n",
    "                    })\n",
    "\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    ic()\n",
    "                    ic(f\"stdout: {e.stdout}\")\n",
    "                    ic(f\"stderr: {e.stderr}\")\n",
    "                    return []\n",
    "                except Exception as e:\n",
    "                    ic()\n",
    "                    ic(e)\n",
    "                    return []\n",
    "\n",
    "        return table_results\n",
    "\n",
    "    @classmethod\n",
    "    def map_os(cls):\n",
    "        os_mapping_vars = cls.settings.get(\"os_mapping\")\n",
    "        tree_command = os_mapping_vars.get(\"tree_command\", None)\n",
    "        tree_file_path = os_mapping_vars.get(\"tree_file_path\", \"../database/system_tree.json\")\n",
    "        delete_tree_command = os_mapping_vars.get(\"delete_tree_command\", None)\n",
    "\n",
    "        if any(var is None for var in [tree_command, tree_file_path, delete_tree_command]):\n",
    "            ic()\n",
    "            ic(\"Fucking Issues!!!\")\n",
    "            return {}\n",
    "\n",
    "        tree_command.append(tree_file_path)\n",
    "        delete_tree_command.append(tree_file_path)\n",
    "\n",
    "        if tree_command:\n",
    "            try:\n",
    "                # Try to delete existing tree file\n",
    "                subprocess.run(\n",
    "                    delete_tree_command,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    check=True\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    # Generate new tree file\n",
    "                    subprocess.run(\n",
    "                        tree_command,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        check=True\n",
    "                    )\n",
    "                    # Process the generated tree file\n",
    "                    return cls.process_os_mapping(tree_file_path)\n",
    "\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    ic()\n",
    "                    ic(f\"stdout: {e.stdout}\")\n",
    "                    ic(f\"stderr: {e.stderr}\")\n",
    "                    return {}\n",
    "                except Exception as e:\n",
    "                    ic()\n",
    "                    ic(f\"error: {e}\")\n",
    "                    return {}\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                # If deletion fails, try to generate anyway\n",
    "                try:\n",
    "                    subprocess.run(\n",
    "                        tree_command,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        check=True\n",
    "                    )\n",
    "                    # Process the generated tree file\n",
    "                    return cls.process_os_mapping(tree_file_path)\n",
    "\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    ic()\n",
    "                    ic(f\"stdout: {e.stdout}\")\n",
    "                    ic(f\"stderr: {e.stderr}\")\n",
    "                    return {}\n",
    "                except Exception as e:\n",
    "                    ic()\n",
    "                    ic(f\"error: {e}\")\n",
    "                    return {}\n",
    "\n",
    "            except Exception as e:\n",
    "                # If another exception occurs during deletion, try to generate anyway\n",
    "                try:\n",
    "                    subprocess.run(\n",
    "                        tree_command,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        check=True\n",
    "                    )\n",
    "                    # Process the generated tree file\n",
    "                    return cls.process_os_mapping(tree_file_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    ic()\n",
    "                    ic(f\"error: {e}\")\n",
    "                    return {}\n",
    "\n",
    "        else:\n",
    "            ic()\n",
    "            ic(\"Fucking Else!#############################\")\n",
    "            return {}\n",
    "\n",
    "    @classmethod\n",
    "    def process_os_mapping_old(cls, tree_file_path=None):\n",
    "        if tree_file_path is None:\n",
    "            tree_file_path = cls.settings.get(\"tree_file_path\", \"../database/system_tree.json\")\n",
    "\n",
    "        try:\n",
    "            with open(tree_file_path, 'r') as file:\n",
    "                json_output = json.load(file)\n",
    "\n",
    "            base_tree, report = json_output\n",
    "            root_dirs = base_tree.get('contents', None)\n",
    "\n",
    "            if not root_dirs:\n",
    "                ic()\n",
    "                ic(f\"Empty root directory list: {root_dirs}\")\n",
    "                return {}\n",
    "\n",
    "            directory_dict = {}\n",
    "            empty_directories = 0\n",
    "            directories = []\n",
    "\n",
    "            for root_dir in root_dirs:\n",
    "                if root_dir[\"type\"] == \"directory\":\n",
    "                    directories.append(root_dir[\"name\"])\n",
    "                if root_dir[\"type\"] == \"directory\" and root_dir.get(\"contents\", False):\n",
    "                    directory_dict[root_dir[\"name\"]] = [item[\"name\"] for item in root_dir[\"contents\"] if\n",
    "                                                        item[\"type\"] == \"file\"]\n",
    "                    root_dirs.extend([item for item in root_dir[\"contents\"] if item[\"type\"] == \"directory\"])\n",
    "\n",
    "                elif root_dir[\"type\"] == \"directory\" and not root_dir.get(\"contents\", False):\n",
    "                    directory_dict[root_dir[\"name\"]] = []\n",
    "                    empty_directories += 1\n",
    "\n",
    "            biggie_count = 0\n",
    "            for path, content in directory_dict.items():\n",
    "                if len(content) > 2000:\n",
    "                    biggie_count += 1\n",
    "\n",
    "            ic()\n",
    "            ic(biggie_count)\n",
    "\n",
    "            return directory_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            ic()\n",
    "            ic(f\"error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    @classmethod\n",
    "    def process_os_mapping(cls, tree_file_path=None):\n",
    "        if tree_file_path is None:\n",
    "            tree_file_path = cls.settings.get(\"tree_file_path\", \"./database/system_tree.json\")\n",
    "\n",
    "        try:\n",
    "            with open(tree_file_path, 'r') as file:\n",
    "                json_output = json.load(file)\n",
    "\n",
    "            base_tree, report = json_output\n",
    "            root_dirs = base_tree.get('contents', None)\n",
    "\n",
    "            if not root_dirs:\n",
    "                ic()\n",
    "                ic(f\"Empty root directory list: {root_dirs}\")\n",
    "                return {}\n",
    "\n",
    "            directory_dict = {}\n",
    "            empty_directories = 0\n",
    "            directories = []\n",
    "\n",
    "            for directory in root_dirs:\n",
    "                if type(directory) == dict:\n",
    "                    if directory[\"type\"] == \"directory\":\n",
    "                        directories.append(directory[\"name\"])\n",
    "                    if directory[\"type\"] == \"directory\" and directory.get(\"contents\", False):\n",
    "                        for item in directory[\"contents\"]:\n",
    "                            if item[\"type\"] == \"file\":\n",
    "                                item_name = item[\"name\"].split(\"/\")[-1]\n",
    "                                directory_dict[item[\"name\"]] = {\"filetype\": item[\"type\"], \"item\": item_name}\n",
    "                            elif item[\"type\"] == \"directory\":\n",
    "                                root_dirs.append(item)\n",
    "\n",
    "\n",
    "                elif directory[\"type\"] == \"directory\" and not directory.get(\"contents\", False):\n",
    "                        item_name = directory[\"name\"].split(\"/\")[-1]\n",
    "                        directory_dict[directory[\"name\"]] = {\"filetype\": directory[\"type\"], \"item\": item_name}\n",
    "                        empty_directories += 1\n",
    "\n",
    "            # Remove Big Items for now. Split up later on.\n",
    "            for path, content in directory_dict.items():\n",
    "                if len(content) > 2000:\n",
    "                    del directory_dict[path]\n",
    "\n",
    "            ic()\n",
    "            ic(len(directory_dict))\n",
    "\n",
    "            return directory_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            ic()\n",
    "            ic(f\"error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def fast_process_os_mapping(self, tree_file_path):\n",
    "        data, _ = json.loads(open(tree_file_path, 'rb').read())\n",
    "        queue = deque(data.get(\"contents\", []))\n",
    "        directory_dict = {}\n",
    "\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            if node[\"type\"] != \"directory\":\n",
    "                continue\n",
    "\n",
    "            # collect file names in this directory\n",
    "            files = [c[\"name\"] for c in node.get(\"contents\", [])\n",
    "                     if c[\"type\"] == \"file\"]\n",
    "            directory_dict[node[\"name\"]] = files\n",
    "\n",
    "            # enqueue subdirectories\n",
    "            for c in node.get(\"contents\", []):\n",
    "                if c[\"type\"] == \"directory\":\n",
    "                    queue.append(c)\n",
    "\n",
    "        return directory_dict\n"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T08:12:10.563387Z",
     "start_time": "2025-05-01T08:12:01.168050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your system tree file (adjust if necessary)\n",
    "tree_file_path = \"../database/system_tree.json\"\n",
    "\n",
    "# Measure time for process_os_mapping\n",
    "print(\"Start process_os_mapping...\")\n",
    "start_time = time.time()\n",
    "result_process_os = SystemMapping.process_os_mapping(tree_file_path)\n",
    "end_time = time.time()\n",
    "process_os_mapping_time = end_time - start_time\n",
    "\n",
    "# Measure time for fast_process_os_mapping\n",
    "print(\"Start fast_process_os_mapping...\")\n",
    "system_mapping_instance = SystemMapping()  # fast_process_os_mapping is an instance method\n",
    "start_time = time.time()\n",
    "result_process_os_old = system_mapping_instance.process_os_mapping_old(tree_file_path)\n",
    "end_time = time.time()\n",
    "fast_process_os_mapping_time = end_time - start_time\n",
    "\n",
    "# Print the timing results\n",
    "print(f\"Time taken by process_os_mapping: {process_os_mapping_time:.6f} seconds\")\n",
    "print(f\"Time taken by fast_process_os_mapping: {fast_process_os_mapping_time:.6f} seconds\")\n",
    "\n",
    "# Optional: Check if results are identical\n",
    "print(f\"Results are identical: {result_process_os == result_process_os_old}\")"
   ],
   "id": "e6b2be1da6389385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start process_os_mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 572392395.py:236 in process_os_mapping() at 10:12:05.941\n",
      "ic| len(directory_dict): 877198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fast_process_os_mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 572392395.py:182 in process_os_mapping_old() at 10:12:09.974\n",
      "ic| biggie_count: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by process_os_mapping: 5.050888 seconds\n",
      "Time taken by fast_process_os_mapping: 4.328471 seconds\n",
      "Results are identical: False\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T08:12:11.501900Z",
     "start_time": "2025-05-01T08:12:11.490982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result_process_os\n",
    "print(type(result_process_os))"
   ],
   "id": "94f0534c82e1140b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:45:29.111420Z",
     "start_time": "2025-05-01T06:45:28.353405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=\"/home/m/PycharmProjects/terminAl/database/vector_db\",\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "collection = client.get_collection(\"Main_Collection\")\n",
    "num_entries = collection.count()\n",
    "print(f\"Number of entries: {num_entries}\")\n"
   ],
   "id": "f8e1ac29b20abf07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T08:24:01.415Z",
     "start_time": "2025-05-01T08:12:22.959603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# Set up embedding function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"intfloat/multilingual-e5-small\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Prepare list of keys (file paths)\n",
    "file_paths = list(result_process_os.keys())\n",
    "print(len(file_paths))\n",
    "\n",
    "# Embed with progress bar\n",
    "embeddings = []\n",
    "for path in tqdm(file_paths, desc=\"Embedding paths\"):\n",
    "    embed = embedding_function([path])[0]  # Model returns a list\n",
    "    embeddings.append((path, embed))\n",
    "\n",
    "# Example output\n",
    "print(f\"\\nEmbedded {len(embeddings)} paths. First 2 results:\")\n",
    "for path, vector in embeddings[:2]:\n",
    "    print(f\"{path} → Vector dim: {len(vector)}\")\n"
   ],
   "id": "3ad45f6cb6054ce4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding paths:   8%|▊         | 72066/877198 [11:37<2:09:57, 103.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[104]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     18\u001B[39m embeddings = []\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m tqdm(file_paths, desc=\u001B[33m\"\u001B[39m\u001B[33mEmbedding paths\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m     embed = \u001B[43membedding_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# Model returns a list\u001B[39;00m\n\u001B[32m     21\u001B[39m     embeddings.append((path, embed))\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# Example output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/chromadb/api/types.py:466\u001B[39m, in \u001B[36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    465\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m: EmbeddingFunction[D], \u001B[38;5;28minput\u001B[39m: D) -> Embeddings:\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m     result = \u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    468\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m validate_embeddings(cast(Embeddings, normalize_embeddings(result)))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/chromadb/utils/embedding_functions/sentence_transformer_embedding_function.py:48\u001B[39m, in \u001B[36mSentenceTransformerEmbeddingFunction.__call__\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Documents) -> Embeddings:\n\u001B[32m     44\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m     45\u001B[39m         Embeddings,\n\u001B[32m     46\u001B[39m         [\n\u001B[32m     47\u001B[39m             embedding\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m             \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     50\u001B[39m \u001B[43m                \u001B[49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m                \u001B[49m\u001B[43mnormalize_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_normalize_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m         ],\n\u001B[32m     54\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:623\u001B[39m, in \u001B[36mSentenceTransformer.encode\u001B[39m\u001B[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001B[39m\n\u001B[32m    620\u001B[39m features.update(extra_features)\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m623\u001B[39m     out_features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    624\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device.type == \u001B[33m\"\u001B[39m\u001B[33mhpu\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    625\u001B[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:690\u001B[39m, in \u001B[36mSentenceTransformer.forward\u001B[39m\u001B[34m(self, input, **kwargs)\u001B[39m\n\u001B[32m    688\u001B[39m     module_kwarg_keys = \u001B[38;5;28mself\u001B[39m.module_kwargs.get(module_name, [])\n\u001B[32m    689\u001B[39m     module_kwargs = {key: value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m kwargs.items() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m module_kwarg_keys}\n\u001B[32m--> \u001B[39m\u001B[32m690\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodule_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    691\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:442\u001B[39m, in \u001B[36mTransformer.forward\u001B[39m\u001B[34m(self, features, **kwargs)\u001B[39m\n\u001B[32m    435\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001B[39;00m\n\u001B[32m    436\u001B[39m trans_features = {\n\u001B[32m    437\u001B[39m     key: value\n\u001B[32m    438\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m features.items()\n\u001B[32m    439\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mtoken_type_ids\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33minputs_embeds\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    440\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m442\u001B[39m output_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mauto_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtrans_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    443\u001B[39m output_tokens = output_states[\u001B[32m0\u001B[39m]\n\u001B[32m    445\u001B[39m \u001B[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001B[39;00m\n\u001B[32m    446\u001B[39m \u001B[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001B[39m, in \u001B[36mBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1135\u001B[39m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[32m   1136\u001B[39m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[32m   1137\u001B[39m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[32m   1138\u001B[39m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[32m   1139\u001B[39m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[32m   1140\u001B[39m head_mask = \u001B[38;5;28mself\u001B[39m.get_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers)\n\u001B[32m-> \u001B[39m\u001B[32m1142\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1143\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1144\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1145\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1146\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1147\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1148\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1149\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1150\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1151\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1152\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1153\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1154\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1155\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.pooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001B[39m, in \u001B[36mBertEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    684\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    685\u001B[39m         layer_module.\u001B[34m__call__\u001B[39m,\n\u001B[32m    686\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    692\u001B[39m         output_attentions,\n\u001B[32m    693\u001B[39m     )\n\u001B[32m    694\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m695\u001B[39m     layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    696\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    697\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    698\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    699\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    700\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    701\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    702\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    703\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    705\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    706\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001B[39m, in \u001B[36mBertLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    574\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    575\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    582\u001B[39m ) -> Tuple[torch.Tensor]:\n\u001B[32m    583\u001B[39m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[32m    584\u001B[39m     self_attn_past_key_value = past_key_value[:\u001B[32m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m585\u001B[39m     self_attention_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    586\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    587\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    588\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    589\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    590\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    592\u001B[39m     attention_output = self_attention_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    594\u001B[39m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:524\u001B[39m, in \u001B[36mBertAttention.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[39m\n\u001B[32m    505\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    506\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    507\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    513\u001B[39m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    514\u001B[39m ) -> Tuple[torch.Tensor]:\n\u001B[32m    515\u001B[39m     self_outputs = \u001B[38;5;28mself\u001B[39m.self(\n\u001B[32m    516\u001B[39m         hidden_states,\n\u001B[32m    517\u001B[39m         attention_mask,\n\u001B[32m   (...)\u001B[39m\u001B[32m    522\u001B[39m         output_attentions,\n\u001B[32m    523\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m524\u001B[39m     attention_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mself_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    525\u001B[39m     outputs = (attention_output,) + self_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n\u001B[32m    526\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:468\u001B[39m, in \u001B[36mBertSelfOutput.forward\u001B[39m\u001B[34m(self, hidden_states, input_tensor)\u001B[39m\n\u001B[32m    466\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.dense(hidden_states)\n\u001B[32m    467\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.dropout(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m468\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mLayerNorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    469\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/terminAl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1735\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1732\u001B[39m             tracing_state.pop_scope()\n\u001B[32m   1733\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m-> \u001B[39m\u001B[32m1735\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_wrapped_call_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m   1736\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1737\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# Your input data\n",
    "# result_process_os = {'/path/file1': {...}, '/path/file2': {...}, ...}\n",
    "# Replace this with your actual data source\n",
    "result_process_os = {\n",
    "    \"/etc/nginx/nginx.conf\": {\"filetype\": \"file\", \"item\": \"nginx.conf\"},\n",
    "    \"/usr/bin/python3\": {\"filetype\": \"file\", \"item\": \"python3\"},\n",
    "    \"/var/log/syslog\": {\"filetype\": \"file\", \"item\": \"syslog\"},\n",
    "}\n",
    "\n",
    "# -------------------\n",
    "# 1. Set up embedding\n",
    "# -------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"intfloat/multilingual-e5-small\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "file_paths = list(result_process_os.keys())\n",
    "print(f\"Total files: {len(file_paths)}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Compute embeddings only\n",
    "# --------------------------\n",
    "embeddings = []\n",
    "for path in tqdm(file_paths, desc=\"Embedding paths\"):\n",
    "    emb = embedding_function([path])[0]\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Insert into ChromaDB cleanly\n",
    "# -----------------------------\n",
    "# Set ChromaDB persistent path\n",
    "CHROMA_DB_PATH = \"./chroma_store\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH, settings=Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Recreate the collection cleanly\n",
    "collection_name = \"test_collection\"\n",
    "try:\n",
    "    client.delete_collection(collection_name)\n",
    "except:\n",
    "    pass  # Collection may not exist yet\n",
    "\n",
    "collection = client.create_collection(name=collection_name)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Prepare metadata & IDs\n",
    "# --------------------------\n",
    "metadatas = [\n",
    "    {\n",
    "        \"filetype\": meta[\"filetype\"],\n",
    "        \"item\": meta[\"item\"],\n",
    "        \"tool\": \"bash\"\n",
    "    } for meta in result_process_os.values()\n",
    "]\n",
    "\n",
    "ids = [str(i) for i in range(len(file_paths))]\n",
    "\n",
    "# --------------------------\n",
    "# 5. Batch insert (recommended)\n",
    "# --------------------------\n",
    "batch_size = 1000\n",
    "for i in tqdm(range(0, len(file_paths), batch_size), desc=\"Uploading to ChromaDB\"):\n",
    "    start, end = i, i + batch_size\n",
    "    collection.add(\n",
    "        documents=file_paths[start:end],\n",
    "        embeddings=embeddings[start:end],\n",
    "        metadatas=metadatas[start:end],\n",
    "        ids=ids[start:end]\n",
    "    )\n",
    "\n",
    "print(f\"\\n✅ Successfully inserted {len(file_paths)} documents into '{collection_name}'.\")\n"
   ],
   "id": "9300bec78f277575"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
